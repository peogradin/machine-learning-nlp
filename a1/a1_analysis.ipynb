{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, nltk, pickle\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from transformers import BatchEncoding, PretrainedConfig, PreTrainedModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sys, time, os\n",
    "from a1.A1_skeleton import A1RNNModel, A1Tokenizer, lowercase_tokenizer\n",
    "import a1.A1_skeleton as a1mod\n",
    "\n",
    "# expose the class under __main__ so pickle can find it\n",
    "sys.modules['__main__'].A1Tokenizer = a1mod.A1Tokenizer\n",
    "sys.modules['__main__'].lowercase_tokenizer = a1mod.lowercase_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A1RNNModel.from_pretrained('a1/a1_rnn_model_trained')\n",
    "tokenizer = A1Tokenizer.from_file(\"a1/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf63d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word: the Value: 5.743141174316406\n",
       "Word: , Value: 5.24702262878418\n",
       "Word: in Value: 5.008247375488281\n",
       "Word: <UNK> Value: 4.727141380310059\n",
       "Word: is Value: 4.545037269592285\n",
       "Word: with Value: 4.3523850440979\n",
       "Word: a Value: 3.768798828125\n",
       "Word: 's Value: 3.358069658279419\n",
       "Word: and Value: 3.32440185546875\n",
       "Word: not Value: 3.288217782974243\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict next word \n",
    "\n",
    "def predict_next_word(model, tokenizer, text, top_k=5):\n",
    "    model.eval()\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([text], return_tensors='pt', padding=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # Get the logits for the last token\n",
    "    logits = outputs[:, -1, :]\n",
    "    \n",
    "    # Get the top k probabilities and their corresponding token IDs\n",
    "    top_k_probs, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "\n",
    "    # Convert token IDs to words using inv_vocabulary\n",
    "    top_k_words = [tokenizer.inv_vocabulary[idx.item()] for idx in top_k_indices[0]]\n",
    "\n",
    "    # print the top k words with their probabilities\n",
    "    print(f\"Next word predictions for {text}:\")\n",
    "    for word, prob in zip(top_k_words, top_k_probs[0]):\n",
    "        print(f\"Word: {word} Value: {prob.item()}\")\n",
    "\n",
    "predict_next_word(model, tokenizer, \"What is\", top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260bf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
